import logging
import random
import threading

import tqdm

import dspy
from dspy.teleprompt.teleprompt import Teleprompter

from .vanilla import LabeledFewShot

logger = logging.getLogger(__name__)

class BootstrapFewShot(Teleprompter):
    def __init__(
        self,
        metric=None,
        metric_threshold=None,
        teacher_settings: dict | None = None,
        max_bootstrapped_demos=4,
        max_labeled_demos=16,
        max_rounds=1,
        max_errors=None,
    ):
        """A Teleprompter that builds a prompt's demos/examples by combining both labeled data and bootstrapped generations.

        Args:
            metric (Callable): A function that compares an expected value and predicted value, returning True (accepted demo) or False (rejected) or a float that will be compared to `metric_threshold` if given.
            metric_threshold (float, optional): If the metric yields a numerical value, this threshold determines whether a generated example is accepted as a demo. Defaults to None (any True-valued result is accepted).
            teacher_settings (dict, optional): Settings for the `teacher` (e.g., alternative LM configuration). Defaults to None.
            max_bootstrapped_demos (int): Max number of bootstrapped demos (generated by the teacher) to include. Defaults to 4.
            max_labeled_demos (int): Max number of labeled demos (from trainset) to include. Defaults to 16.
            max_rounds (int): Number of times to iterate through trainset looking for working bootstraps. Defaults to 1 (single pass).
            max_errors (int, optional): Maximum number of errors allowed before halting. If not set, uses dspy.settings.max_errors. Defaults to None.
        """
        self.metric = metric
        self.metric_threshold = metric_threshold
        self.teacher_settings = {} if teacher_settings is None else teacher_settings

        self.max_bootstrapped_demos = max_bootstrapped_demos
        self.max_labeled_demos = max_labeled_demos
        self.max_rounds = max_rounds
        self.max_errors = max_errors
        self.error_count = 0
        self.error_lock = threading.Lock()

    def compile(self, student, *, teacher=None, trainset):
        """Compiles optimized demos for the student via a mixture of labeled and bootstrapped examples.
        Ensures that the student and teacher have compatible predictors and signatures, and handles error tolerance.
        """
        self.trainset = trainset

        self._prepare_student_and_teacher(student, teacher)
        self._prepare_predictor_mappings()
        self._bootstrap()

        self.student = self._train()
        self.student._compiled = True

        return self.student

    def _prepare_student_and_teacher(self, student, teacher):
        self.student = student.reset_copy()
        # Use deepcopy for teacher to avoid side-effects from sharing the student.
        self.teacher = teacher.deepcopy() if teacher is not None else student.deepcopy()
        assert getattr(self.student, "_compiled", False) is False, "Student must be uncompiled."
        if self.max_labeled_demos and getattr(self.teacher, "_compiled", False) is False:
            teleprompter = LabeledFewShot(k=self.max_labeled_demos)
            self.teacher = teleprompter.compile(self.teacher.reset_copy(), trainset=self.trainset)

    def _prepare_predictor_mappings(self):
        """Ensures one-to-one mapping of predictors in student and teacher."""
        name2predictor, predictor2name = {}, {}
        student, teacher = self.student, self.teacher
        assert len(student.predictors()) == len(
            teacher.predictors(),
        ), "Student and teacher must have the same number of predictors."
        for (name1, predictor1), (name2, predictor2) in zip(
            student.named_predictors(), teacher.named_predictors(), strict=False
        ):
            assert name1 == name2, "Student and teacher must have the same program structure."
            if hasattr(predictor1.signature, "equals"):
                assert predictor1.signature.equals(
                    predictor2.signature,
                ), (
                    f"Student and teacher must have the same signatures. "
                    f"{type(predictor1.signature)} != {type(predictor2.signature)}"
                )
            else:
                # fallback in case if .equals is not implemented (e.g. dsp.Prompt)
                assert predictor1.signature == predictor2.signature, (
                    f"Student and teacher must have the same signatures. "
                    f"{type(predictor1.signature)} != {type(predictor2.signature)}"
                )
            assert id(predictor1) != id(predictor2), "Student and teacher must be different objects."
            name2predictor[name1] = None
            predictor2name[id(predictor1)] = name1
            predictor2name[id(predictor2)] = name2
        self.name2predictor = name2predictor
        self.predictor2name = predictor2name

    def _bootstrap(self, *, max_bootstraps=None):
        """Attempts to gather up to `max_bootstraps` examples from the trainset that pass the metric, considering max_rounds and tolerant of up to max_errors before halting."""
        max_bootstraps = max_bootstraps or self.max_bootstrapped_demos
        bootstrap_attempts = 0
        bootstrapped = {}
        self.name2traces = {name: [] for name in self.name2predictor}
        for example_idx, example in enumerate(tqdm.tqdm(self.trainset)):
            if len(bootstrapped) >= max_bootstraps:
                break
            for round_idx in range(self.max_rounds):
                bootstrap_attempts += 1
                if self._bootstrap_one_example(example, round_idx):
                    bootstrapped[example_idx] = True
                    break
        print(
            f"Bootstrapped {len(bootstrapped)} full traces after {example_idx} examples "
            f"for up to {self.max_rounds} rounds, amounting to {bootstrap_attempts} attempts."
        )
        self.validation = [x for idx, x in enumerate(self.trainset) if idx not in bootstrapped]
        random.Random(0).shuffle(self.validation)
        self.validation = self.validation

    def _bootstrap_one_example(self, example, round_idx=0):
        """Evaluates a single example for candidate demos, including error tolerance and collecting passes per predictor."""
        name2traces = {}
        teacher = self.teacher
        predictor_cache = {}
        try:
            with dspy.settings.context(trace=[], **self.teacher_settings):
                lm = dspy.settings.lm
                lm = lm.copy(temperature=0.7 + 0.001 * round_idx) if round_idx > 0 else lm
                new_settings = {"lm": lm} if round_idx > 0 else {}
                with dspy.settings.context(**new_settings):
                    for name, predictor in teacher.named_predictors():
                        predictor_cache[name] = predictor.demos
                        predictor.demos = [x for x in predictor.demos if x != example]
                    prediction = teacher(**example.inputs())
                    trace = dspy.settings.trace
                    for name, predictor in teacher.named_predictors():
                        predictor.demos = predictor_cache[name]
                if self.metric:
                    metric_val = self.metric(example, prediction, trace)
                    if self.metric_threshold:
                        success = metric_val >= self.metric_threshold
                    else:
                        success = metric_val
                else:
                    success = True
        except Exception as e:
            success = False
            with self.error_lock:
                self.error_count += 1
                current_error_count = self.error_count
            effective_max_errors = self.max_errors if self.max_errors is not None else dspy.settings.max_errors
            if current_error_count >= effective_max_errors:
                raise e
            logger.error(f"Failed to run or to evaluate example {example} with {self.metric} due to {e}.")
        if success:
            for step in trace:
                predictor, inputs, outputs = step
                demo = dspy.Example(augmented=True, **inputs, **outputs)
                try:
                    predictor_name = self.predictor2name[id(predictor)]
                except KeyError:
                    continue  # FIXME: !
                name2traces[predictor_name] = name2traces.get(predictor_name, [])
                name2traces[predictor_name].append(demo)
            for name, demos in name2traces.items():
                # If there are multiple traces for the same predictor in the sample example,
                # sample 50/50 from the first N-1 traces or the last trace.
                if len(demos) > 1:
                    from dspy.utils.hasher import Hasher
                    rng = random.Random(Hasher.hash(tuple(demos)))
                    demos = [rng.choice(demos[:-1]) if rng.random() < 0.5 else demos[-1]]
                self.name2traces[name].extend(demos)
        return success

    def _train(self):
        rng = random.Random(0)
        raw_demos = self.validation
        for name, predictor in self.student.named_predictors():
            augmented_demos = self.name2traces[name][: self.max_bootstrapped_demos]
            sample_size = min(self.max_labeled_demos - len(augmented_demos), len(raw_demos))
            sample_size = max(0, sample_size)
            raw_demos = rng.sample(raw_demos, sample_size)
            predictor.demos = augmented_demos + raw_demos
        return self.student
